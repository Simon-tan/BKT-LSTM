{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "#import tensorflow.python.util.deprecation as deprecation\n",
    "#deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from tensorflow.contrib import rnn\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "import os\n",
    "from numpy import array\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.stats as ss\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from math import log\n",
    "\n",
    "from BKT import BKT\n",
    "\n",
    "model_name = 'BKT-LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\",1e-2, \"Learning rate\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 20.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\", 0.6, \"Keep probability for dropout\")\n",
    "tf.flags.DEFINE_integer(\"hidden_layer_num\", 1, \"The number of hidden layers (Integer)\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"problem_len\", 20, \"length for each time interval\")\n",
    "tf.flags.DEFINE_integer(\"num_cluster\", 7, \"length for each time interval\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 40, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.flags.DEFINE_string(\"model_name\", model_name, \"model used\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "\n",
    "def add_gradient_noise(t, stddev=1e-3, name=None):\n",
    "\n",
    "    with tf.op_scope([t, stddev], name, \"add_gradient_noise\") as name:\n",
    "        t = tf.convert_to_tensor(t, name=\"t\")\n",
    "        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n",
    "        return tf.add(t, gn, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(object):\n",
    "\n",
    "    def __init__(self, is_training, config):\n",
    "        self._batch_size = batch_size = config.batch_size\n",
    "        self.num_skills = num_skills = config.num_skills \n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        \n",
    "        \n",
    "        label_size = (num_skills*2)\n",
    "        id_size = num_skills \n",
    "        df_size = 11\n",
    "        cluster_size=(FLAGS.num_cluster+1)        \n",
    "        reuse_flag=False\n",
    "        \n",
    "        output_size = (num_skills+1)\n",
    "        \n",
    "        \n",
    "        self.cluster = tf.placeholder(tf.int32, [batch_size, num_steps], name='profile')\n",
    "        self.diff = tf.placeholder(tf.int32, [batch_size, num_steps], name='difficulty')\n",
    "        self.assess = tf.placeholder(tf.float32, [batch_size, num_steps], name='Assessment')\n",
    "        \n",
    "        self._target_id = target_id = tf.placeholder(tf.int32, [None])\n",
    "        self._target_correctness = target_correctness = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        \n",
    "        #one-hot encoding\n",
    "        \n",
    "        cluster = tf.reshape(self.cluster, [-1])\n",
    "        slice_cluster_data = one_hot_output(cluster, cluster_size, batch_size, num_steps)\n",
    "        \n",
    "        diff = tf.reshape(self.diff, [-1])\n",
    "        slice_diff_data = one_hot_output(diff, df_size, batch_size, num_steps)\n",
    "        \n",
    "        assess = tf.reshape(self.assess, [batch_size, num_steps, 1])\n",
    "        slice_ass_data = tf.split(assess, num_steps, 1)\n",
    "        \n",
    "        input_l = []\n",
    "        for i in range(num_steps): \n",
    "            if i != 0:\n",
    "               reuse_flag = True\n",
    "                        \n",
    "            \n",
    "            cu = tf.squeeze(slice_cluster_data[i], 1)\n",
    "            df = tf.squeeze(slice_diff_data[i], 1)\n",
    "            ass =  tf.squeeze(slice_ass_data[i], 1)\n",
    "            ## Concat [Assessment, Profile, Difficulty]\n",
    "            m1 = tf.concat([ass, cu, df], 1) \n",
    "            input_l.append(m1)            \n",
    "            \n",
    "            \n",
    "            \n",
    "        input_= tf.stack(input_l)        \n",
    "        input_size=int(input_[0].get_shape()[1])\n",
    "        x_input = tf.reshape(input_, [-1, input_size])        \n",
    "        x_input = tf.split(x_input, num_steps, 0)\n",
    "        \n",
    "        final_hidden_size = input_size        \n",
    "        hidden_layers = []\n",
    "        for i in range(FLAGS.hidden_layer_num):\n",
    "            final_hidden_size = final_hidden_size\n",
    "            hidden1 = tf.nn.rnn_cell.LSTMCell(final_hidden_size, state_is_tuple=True)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                hidden1 = tf.nn.rnn_cell.DropoutWrapper(hidden1, output_keep_prob=FLAGS.keep_prob)\n",
    "            hidden_layers.append(hidden1)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(hidden_layers, state_is_tuple=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs, state = rnn.static_rnn(cell, x_input, dtype=tf.float32)\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, int(final_hidden_size)])\n",
    "        sigmoid_w = tf.get_variable(\"sigmoid_w\", [final_hidden_size, output_size])\n",
    "        sigmoid_b = tf.get_variable(\"sigmoid_b\", [output_size])\n",
    "        logits = tf.matmul(output, sigmoid_w) + sigmoid_b\n",
    "        logits = tf.reshape(logits, [-1])        \n",
    "        selected_logits = tf.gather(logits, self.target_id)\n",
    "        self._all_logits = tf.sigmoid(logits)\n",
    "\n",
    "        #make prediction\n",
    "        self._pred = tf.sigmoid(selected_logits)\n",
    "\n",
    "        # loss function\n",
    "        loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=selected_logits, labels=target_correctness))\n",
    "        self._cost = cost = loss\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def auc(self):\n",
    "        return self._auc\n",
    "\n",
    "    @property\n",
    "    def pred(self):\n",
    "        return self._pred\n",
    "\n",
    "    @property\n",
    "    def target_id(self):\n",
    "        return self._target_id\n",
    "\n",
    "    @property\n",
    "    def target_correctness(self):\n",
    "        return self._target_correctness\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def all_logits(self):\n",
    "        return self._all_logits\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clust(session, train_students, test_students, max_stu, max_seg, num_clust, num_skills, num_iter):\n",
    "    identifiers=3\n",
    "    max_stu=int(max_stu)\n",
    "    max_seg=int(max_seg)\n",
    "    cluster= np.zeros((max_stu,max_seg))\n",
    "    data=[]\n",
    "    for ind,i in enumerate(train_students):\n",
    "        data.append(i[:-identifiers])\n",
    "    data = array(data)\n",
    "    points = tf.constant(data)  \n",
    "\n",
    "    centroids = tf.Variable(tf.random_shuffle(points)[:num_clust, :])\n",
    "    # calculate distances from the centroids to each point\n",
    "    points_e = tf.expand_dims(points, axis=0) # (1, N, 2)\n",
    "    centroids_e = tf.expand_dims(centroids, axis=1) # (k, 1, 2)  \n",
    "    distances = tf.reduce_sum((points_e - centroids_e) ** 2, axis=-1) # (k, N)\n",
    "    # find the index to the nearest centroids from each point\n",
    "    indices = tf.argmin(distances, axis=0) # (N,)\n",
    "    # gather k clusters: list of tensors of shape (N_i, 1, 2) for each i\n",
    "    clusters = [tf.gather(points, tf.where(tf.equal(indices, i))) for i in range(num_clust)]\n",
    "    # get new centroids (k, 2)\n",
    "    new_centroids = tf.concat([tf.reduce_mean(clusters[i], reduction_indices=[0]) for i in range(num_clust)], axis=0)\n",
    "    # update centroids\n",
    "    assign = tf.assign(centroids, new_centroids)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for j in range(num_iter):\n",
    "        clusters_val, centroids_val, _ = session.run([clusters, centroids, assign])\n",
    "        \n",
    "    \n",
    "    for ind,i in enumerate(train_students):\n",
    "        inst=i[:-identifiers]\n",
    "        min_dist=float('inf')\n",
    "        closest_clust=None            \n",
    "        for j in range(num_clust):\n",
    "            if euclideanDistance(inst,centroids_val[j])< min_dist:\n",
    "               cur_dist=euclideanDistance(inst,centroids_val[j])\n",
    "               if cur_dist<min_dist:                  \n",
    "                  min_dist=cur_dist\n",
    "                  closest_clust=j                  \n",
    "        \n",
    "        cluster[int(i[-2]),int(i[-1])]=closest_clust\n",
    "        \n",
    "   \n",
    "    for ind,i in enumerate(test_students):\n",
    "        inst=i[:-identifiers]\n",
    "        min_dist=float('inf')\n",
    "        closest_clust=None \n",
    "        for j in range(num_clust):\n",
    "            if euclideanDistance(inst,centroids_val[j])< min_dist:\n",
    "               cur_dist=euclideanDistance(inst,centroids_val[j])\n",
    "               if cur_dist<min_dist:\n",
    "                  min_dist=cur_dist\n",
    "                  closest_clust=j\n",
    "        cluster[int(i[-2]),int(i[-1])]=closest_clust\n",
    "        \n",
    "        \n",
    "    del train_students, test_students\n",
    "    \n",
    "    return cluster\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_output(input_data, output_size, batch_size, num_steps):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "         labels = tf.expand_dims(input_data, 1)\n",
    "         indices = tf.expand_dims(tf.range(0, batch_size*num_steps, 1), 1)\n",
    "         concated = tf.concat([indices, labels],1)\n",
    "         input_data = tf.sparse_to_dense(concated, tf.stack([batch_size*num_steps, output_size]), 1.0, 0.0)\n",
    "         input_data.set_shape([batch_size*num_steps, output_size])\n",
    "         input_data = tf.reshape(input_data, [batch_size, num_steps, output_size ])\n",
    "         output_data = tf.split(input_data, num_steps, 1)\n",
    "    return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_data(students,max_items):\n",
    "          \n",
    "    limit= 3\n",
    "    xtotal = np.zeros(max_items+1)\n",
    "    x1 = np.zeros(max_items+1)\n",
    "    items=[]\n",
    "    Allitems=[]\n",
    "    item_diff ={} \n",
    "    index=0      \n",
    "    while(index < len(students)):\n",
    "         student = students[index]         \n",
    "         item_ids = student[3]\n",
    "         correctness = student[2]         \n",
    "         for j in range(len(item_ids)):         \n",
    "             \n",
    "             key =item_ids[j]             \n",
    "             xtotal[key] +=1\n",
    "             if(int(correctness[j]) == 0):\n",
    "                x1[key] +=1\n",
    "             if xtotal[key]>limit and key > 0 and key not in items  :\n",
    "                items.append(key)\n",
    "             \n",
    "             if xtotal[key]>0 and key not in Allitems :\n",
    "                Allitems.append(key)\n",
    "                \n",
    "         index+=1\n",
    "    for i in (items):\n",
    "        diff =(np.around(float(x1[i])/float(xtotal[i]), decimals=1)*10).astype(int)   \n",
    "        item_diff[i]=diff\n",
    "    \n",
    "\n",
    "    return item_diff     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, m, students, item_diff, max_stu, cluster, eval_op, num_skills, datatype, epoch, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"   \n",
    "    index = 0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    all_all_logits = []\n",
    "    check_point= verbose \n",
    "        \n",
    "    while(index+m.batch_size < len(students)):\n",
    "        cl = np.zeros((m.batch_size, m.num_steps))\n",
    "        clus = np.zeros((m.batch_size, m.num_steps))\n",
    "        diff = np.zeros((m.batch_size, m.num_steps))\n",
    "        assess=np.zeros((m.batch_size, m.num_steps))\n",
    "       \n",
    "        \n",
    "        target_ids = []\n",
    "        target_correctness = []        \n",
    "        for i in range(m.batch_size):\n",
    "            student = students[index+i]\n",
    "            student_id = student[0][0]\n",
    "            seg_id = int(student[0][1])             \n",
    "            \n",
    "            if (seg_id>0):\n",
    "                cluster_id= cluster[student_id,(seg_id-1)]+1\n",
    "            else:\n",
    "                cluster_id= 0           \n",
    "            \n",
    "            problem_ids = student[1]\n",
    "            correctness = student[2] \n",
    "            items = student[3]       \n",
    "            bkt= student[4]   \n",
    "                        \n",
    "            for j in range(len(problem_ids)-1):\n",
    "                indx= j \n",
    "                \n",
    "                \n",
    "                target_id = int(problem_ids[indx])\n",
    "                item = int(items[indx]) \n",
    "                kcass = float(bkt[indx])  \n",
    "                \n",
    "                current_correct = int(correctness[indx])\n",
    "\n",
    "                # to ignore if target_id is null or -1 all skill index are started from 0\n",
    "                \n",
    "                if target_id > -1:\n",
    "                \n",
    "                   df = 0\n",
    "                   if item in item_diff.keys():                      \n",
    "                      df = int(item_diff[item])\n",
    "                   else:\n",
    "                        df=5\n",
    "                                     \n",
    "                   clus[i,j] = cluster_id  ## Ability Profile\n",
    "                   diff[i,j] = df          ## Problem Difficulty\n",
    "                   assess[i,j] = np.round(kcass,3)   ## Mastery Assessment  \n",
    "                   \n",
    "                   output_size = (m.num_skills+1)                   \n",
    "                   burffer_space=i*m.num_steps*(output_size)+j*(output_size)\n",
    "                   t_ind=burffer_space+ int(target_id)\n",
    "                   target_ids.append(t_ind)\n",
    "                   \n",
    "                    \n",
    "                   target_correctness.append(int(correctness[indx]))                \n",
    "                   actual_labels.append(int(correctness[indx]))\n",
    "                 \n",
    "        index += m.batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred, _, all_logits = session.run([m.pred, eval_op, m.all_logits], feed_dict={\n",
    "            m.cluster: clus, m.diff: diff, m.assess:assess, m.target_id: target_ids, m.target_correctness: target_correctness})\n",
    "        \n",
    "        predicted=[]\n",
    "        for i, p in enumerate(pred):\n",
    "            pred_labels.append(np.nan_to_num(p))\n",
    "            predicted.append(float(np.nan_to_num(p)))\n",
    "\n",
    "        all_all_logits.append(all_logits)\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(actual_labels, pred_labels))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual_labels, pred_labels, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    r2 = r2_score(actual_labels, pred_labels)\n",
    "    \n",
    "    pred_labels=np.array(pred_labels)\n",
    "    pred_labels[pred_labels > 0.5] = 1\n",
    "    pred_labels[pred_labels <= 0.5] = 0\n",
    "    acc=metrics.accuracy_score(actual_labels, pred_labels)\n",
    "    acc=metrics.accuracy_score(actual_labels, pred_labels)\n",
    "    del students\n",
    "    return rmse, auc, r2, acc, np.concatenate(all_all_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_csv_file(trainfile, testfile):\n",
    "    rows = []\n",
    "    max_skills = 0\n",
    "    max_steps = 0 \n",
    "    max_items =0\n",
    "    studentids = []\n",
    "    train_ids=[]\n",
    "    test_ids=[]\n",
    "    \n",
    "    problem_len = 20  \n",
    "    with open(trainfile, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "            \n",
    "            \n",
    "    skill_rows=[]\n",
    "    correct_rows=[]\n",
    "    stu_rows=[]\n",
    "    opp_rows=[]\n",
    "    index = 0\n",
    "    while(index < len(rows)):\n",
    "         if int(rows[index][0])>problem_len: \n",
    "            problems = int(rows[index][0]) \n",
    "            student_id= int(rows[index][1])\n",
    "            train_ids.append(student_id)  \n",
    "            \n",
    "            tmp_max_skills = max(map(int, rows[index+1]))\n",
    "            if(tmp_max_skills > max_skills):\n",
    "               max_skills = tmp_max_skills\n",
    "                        \n",
    "                        \n",
    "            tmp_max_items = max(map(int, rows[index+2]))\n",
    "            if(tmp_max_items > max_items):\n",
    "               max_items = tmp_max_items\n",
    "               \n",
    "            skill_rows=np.append(skill_rows,rows[index+1])\n",
    "            correct_rows=np.append(correct_rows,rows[index+3])\n",
    "            stu_rows=np.append(stu_rows,([student_id]* len(rows[index+1])))\n",
    "            opp_rows=np.append(opp_rows, list(range(len(rows[index+1]))))\n",
    "         index += 4  \n",
    "         \n",
    "         \n",
    "         \n",
    "    with open(testfile, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "            \n",
    "            \n",
    "    \n",
    "    while(index < len(rows)):\n",
    "         if int(rows[index][0])>problem_len: \n",
    "            problems = int(rows[index][0]) \n",
    "            student_id= int(rows[index][1])\n",
    "            test_ids.append(student_id)  \n",
    "            \n",
    "            tmp_max_skills = max(map(int, rows[index+1]))\n",
    "            if(tmp_max_skills > max_skills):\n",
    "               max_skills = tmp_max_skills\n",
    "                        \n",
    "                        \n",
    "            tmp_max_items = max(map(int, rows[index+2]))\n",
    "            if(tmp_max_items > max_items):\n",
    "               max_items = tmp_max_items\n",
    "               \n",
    "            skill_rows=np.append(skill_rows,rows[index+1])\n",
    "            correct_rows=np.append(correct_rows,rows[index+3])\n",
    "            stu_rows=np.append(stu_rows,([student_id]* len(rows[index+1])))\n",
    "            opp_rows=np.append(opp_rows, list(range(len(rows[index+1]))))\n",
    "         index += 4  \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "    max_skills =max_skills+1\n",
    "    max_items = max_items+1     \n",
    "    \n",
    "    data= pd.DataFrame({'stus': stu_rows, 'skills': skill_rows, 'corrects': correct_rows, 'opp': opp_rows}).astype(int)\n",
    "    bkt_ass= BKTAssessment(data, train_ids, max_skills)\n",
    "    \n",
    "    del skill_rows, correct_rows, stu_rows, opp_rows, data\n",
    "    \n",
    "    \n",
    "    index = 0   \n",
    "    tuple_rows = []\n",
    "    while(index < len(rows)):\n",
    "          if int(rows[index][0])>problem_len: \n",
    "                  \n",
    "                  problems = int(rows[index][0]) \n",
    "                  student_id= int(rows[index][1])\n",
    "                  studentids.append(student_id)  \n",
    "                  \n",
    "                                 \n",
    "                  \n",
    "                  \n",
    "                  if (problems>problem_len):\n",
    "                  \n",
    "                  \n",
    "                     tmp_max_steps = int(rows[index][0])\n",
    "                     if(tmp_max_steps > max_steps):\n",
    "                        max_steps = tmp_max_steps\n",
    "                        \n",
    "                        \n",
    "                     asses= bkt_ass[student_id]\n",
    "                                        \n",
    "                     len_problems = int(int(problems) / problem_len)*problem_len\n",
    "                     rest_problems = problems - len_problems             \n",
    "                     \n",
    "                     ele_p = []             \n",
    "                     p_index=0       \n",
    "                     for element in rows[index+1]:\n",
    "                         ele_p.append(int(element))\n",
    "                         p_index=p_index+1 \n",
    "                         \n",
    "                     ele_c = []\n",
    "                     c_index=0\n",
    "                     for element in rows[index+3]:\n",
    "                         ele_c.append(int(element))\n",
    "                         c_index=c_index+1\n",
    "                         \n",
    "                         \n",
    "                     ele_d = []\n",
    "                     d_index=0\n",
    "                     for element in rows[index+2]:\n",
    "                         ele_d.append(int(element))\n",
    "                         d_index=d_index+1\n",
    "                         \n",
    "                         \n",
    "                     ele_a = []             \n",
    "                     a_index=0       \n",
    "                     for element in asses:\n",
    "                         ele_a.append(float(element))\n",
    "                         a_index=a_index+1 \n",
    "\n",
    "                     if (rest_problems>0):\n",
    "                        rest=problem_len-rest_problems\n",
    "                        for i in range(rest):\n",
    "                            ele_p.append(-1)\n",
    "                            ele_c.append(-1)\n",
    "                            ele_d.append(-1)\n",
    "                            ele_a.append(-1)\n",
    "\n",
    "                     ele_p_array = np.reshape(np.asarray(ele_p), (-1,problem_len))\n",
    "                     ele_c_array = np.reshape(np.asarray(ele_c), (-1,problem_len))\n",
    "                     ele_d_array = np.reshape(np.asarray(ele_d), (-1,problem_len))\n",
    "                     ele_a_array = np.reshape(np.asarray(ele_a), (-1,problem_len))\n",
    "                   \n",
    "                     n_pieces = ele_p_array.shape[0]\n",
    "                     \n",
    "                   \n",
    "                     for j in range(n_pieces):\n",
    "                         s1=[student_id,j,problems]\n",
    "                         \n",
    "                         if (j>-1) & (j< (n_pieces-1)) :\n",
    "                            s1.append(1)\n",
    "                            s2= np.append(ele_p_array[j,:],ele_p_array[j+1,0]).tolist()\n",
    "                            s3= np.append(ele_c_array[j,:],ele_c_array[j+1,0]).tolist() \n",
    "                            s4= np.append(ele_d_array[j,:],ele_d_array[j+1,0]).tolist()  \n",
    "                            s5= np.append(ele_a_array[j,:],ele_a_array[j+1,0]).tolist()      \n",
    "                         else:\n",
    "                              s1.append(-1)\n",
    "                              s2= ele_p_array[j,:].tolist()\n",
    "                              s3= ele_c_array[j,:].tolist() \n",
    "                              s4= ele_d_array[j,:].tolist() \n",
    "                              s5= ele_a_array[j,:].tolist() \n",
    "                         tup = (s1,s2,s3,s4,s5)\n",
    "                         tuple_rows.append(tup)\n",
    "          index += 4\n",
    "          \n",
    "          \n",
    "          \n",
    "    \n",
    "    \n",
    "    \n",
    "    max_steps  =max_steps+1\n",
    "    \n",
    "    \n",
    "    index=0\n",
    "    train_students=[]\n",
    "    test_students=[]            \n",
    "    while(index < len(tuple_rows)):\n",
    "         if (int(tuple_rows[index][0][0]) in train_ids):\n",
    "            train_students.append(tuple_rows[index])\n",
    "         if (int(tuple_rows[index][0][0]) in test_ids):\n",
    "            test_students.append(tuple_rows[index])\n",
    "         index+=1\n",
    "    \n",
    "      \n",
    "    return train_students, test_students, studentids, max_skills, max_items, train_ids, test_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bktdata(df): \n",
    "    BKT_dict = {}\n",
    "    DKT_skill_dict = {}\n",
    "    DKT_res_dict = {}   \n",
    "\n",
    "    for kc in list(df['skills'].unique()):\n",
    "        kc_df=df[df['skills']==kc].sort_values(['stus'],ascending=True)             \n",
    "        stu_cfa_dict = {}\n",
    "        \n",
    "        for stu in list(kc_df['stus'].unique()):\n",
    "            df_final=kc_df[kc_df['stus']==int(stu)].reset_index().sort_values(['opp'],ascending=True)\n",
    "            stu_cfa_dict[int(stu)]=list(df_final['corrects'])\n",
    "            \n",
    "        BKT_dict[int(kc)]=stu_cfa_dict\n",
    "        \n",
    "        \n",
    "    for stu in list(df['stus'].unique()):\n",
    "        stu_df=df[df['stus']==int(stu)].sort_values(['opp'],ascending=True)\n",
    "        DKT_skill_dict[int(stu)]=list(stu_df['skills'])\n",
    "        DKT_res_dict[int(stu)]=list(stu_df['corrects'])\n",
    "        \n",
    "    \n",
    "\n",
    "    return BKT_dict, DKT_skill_dict, DKT_res_dict\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(students,max_stu,num_skills, datatype):\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    success = []\n",
    "    max_seg =0    \n",
    "    xtotal = np.zeros((max_stu,num_skills))    \n",
    "    x1 = np.zeros((max_stu,num_skills))\n",
    "    x0 = np.zeros((max_stu,num_skills)) \n",
    "    \n",
    "    index = 0  \n",
    "    while(index+FLAGS.batch_size < len(students)):    \n",
    "         for i in range(FLAGS.batch_size):\n",
    "             student = students[index+i] \n",
    "             student_id = int(student[0][0])\n",
    "             seg_id = int(student[0][1])\n",
    "             \n",
    "                 \n",
    "             if (int(student[0][3])==1):\n",
    "                tmp_seg = seg_id\n",
    "                if(tmp_seg > max_seg):\n",
    "                   max_seg = tmp_seg\n",
    "                problem_ids = student[1]                \n",
    "                correctness = student[2]\n",
    "                for j in range(len(problem_ids)):           \n",
    "                    key =problem_ids[j]\n",
    "                    xtotal[student_id,key] +=1\n",
    "                    if(int(correctness[j]) == 1):\n",
    "                      x1[student_id,key] +=1\n",
    "                    else:\n",
    "                         x0[student_id,key] +=1\n",
    "\n",
    "                xsr=[x/y  for x, y in zip(x1[student_id], xtotal[student_id])]\n",
    "                \n",
    "                x=np.nan_to_num(xsr)\n",
    "                x=np.append(x, student_id)\n",
    "                x=np.append(x, seg_id)\n",
    "                success.append(x) \n",
    "                \n",
    "                \n",
    "         index += FLAGS.batch_size \n",
    "         \n",
    "    return success, max_seg \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(instance1, instance2):\n",
    "    distance = 0\n",
    "    for x in range(len(instance1)):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BKTAssessment(data, train_ids, max_skills):\n",
    "\n",
    "    bkt_data, dkt_skill, dkt_res =get_bktdata(data)\n",
    "    DL, DT, DG, DS ={}, {}, {}, {}\n",
    "    for i in bkt_data.keys():\n",
    "        skill_data = bkt_data[i]\n",
    "        train_data=[]\n",
    "        for j in skill_data.keys():\n",
    "            if int(j) in train_ids:                       \n",
    "               train_data.append(list(map(int,skill_data[j])))\n",
    "                       \n",
    "        bkt = BKT(step = 0.1, bounded = False, best_k0 = True)\n",
    "        if len(train_data)>2:\n",
    "           DL[i],DT[i],DG[i],DS[i]=bkt.fit(train_data)   \n",
    "        else:\n",
    "             DL[i],DT[i],DG[i],DS[i] = 0.5, 0.2, 0.1, 0.1   \n",
    "        \n",
    "    del bkt_data\n",
    "    \n",
    "    mastery =  bkt.inter_predict(dkt_skill, dkt_res, DL, DT, DG, DS, max_skills)\n",
    "    \n",
    "    del dkt_skill, dkt_res\n",
    "    \n",
    "    return mastery\n",
    "    \n",
    "    print(\"**************Finished BKT Assessment****************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParamsConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    num_skills = 0\n",
    "    num_steps = FLAGS.problem_len\n",
    "    batch_size = FLAGS.batch_size\n",
    "    max_grad_norm = FLAGS.max_grad_norm\n",
    "    max_max_epoch = FLAGS.epochs\n",
    "    keep_prob = FLAGS.keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_args):\n",
    "\n",
    "    config = HyperParamsConfig()\n",
    "    \n",
    "    data_name= '4_Ass_09'\n",
    "    cluster_num= FLAGS.num_cluster\n",
    "    problem_len= FLAGS.problem_len\n",
    "    \n",
    "    \n",
    "    train_data='./'+data_name+'_train.csv'\n",
    "    test_data= './'+data_name+'_test.csv'\n",
    "          \n",
    "    train_students, test_students, student_ids, max_skills, max_items, train_ids, test_ids =read_data_from_csv_file(train_data, test_data)\n",
    "    config.num_skills = max_skills\n",
    "                 \n",
    "    \n",
    "    item_diff = difficulty_data(train_students+test_students,max_items)             \n",
    "\n",
    "    train_cluster_data, train_max_seg= cluster_data(train_students,max(train_ids)+1,max_skills,\"train\")        \n",
    "    test_cluster_data, test_max_seg= cluster_data(test_students,max(test_ids)+1,max_skills, \"test\")\n",
    "    \n",
    "    max_stu= max(student_ids)+1\n",
    "    max_seg=max([int(train_max_seg),int(test_max_seg)])+1\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "            \n",
    "    with tf.Graph().as_default():\n",
    "         session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                                       log_device_placement=FLAGS.log_device_placement)\n",
    "         global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "         starter_learning_rate = FLAGS.learning_rate\n",
    "         learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=FLAGS.epsilon)\n",
    "         with tf.Session(config=session_conf) as session:\n",
    "              cluster =k_means_clust(session, train_cluster_data, test_cluster_data, max_stu, max_seg, FLAGS.num_cluster, max_skills, 40)\n",
    "              del train_cluster_data, test_cluster_data\n",
    "              initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "              # training model\n",
    "              with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "                   m = StudentModel(is_training=True, config=config)\n",
    "              # testing model\n",
    "              with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "                   mtest = StudentModel(is_training=False, config=config)\n",
    "              grads_and_vars = optimizer.compute_gradients(m.cost)\n",
    "              grads_and_vars = [(tf.clip_by_norm(g, FLAGS.max_grad_norm), v)\n",
    "              for g, v in grads_and_vars if g is not None]\n",
    "              grads_and_vars = [(add_gradient_noise(g), v) for g, v in grads_and_vars]\n",
    "              train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "              session.run(tf.initialize_all_variables())\n",
    "              j=1\n",
    "              for i in range(config.max_max_epoch):\n",
    "                  rmse, auc, r2, acc, _ = run_epoch(session, m, train_students, item_diff, max_stu, cluster, train_op,max_skills, \"train\", i,  verbose=False)\n",
    "                  print(\" Epoch: %d Train Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2:  %.3f \\t acc: %.3f \\n\" % ( i + 1, rmse, auc, r2, acc))\n",
    "                  if((i+1) % FLAGS.evaluation_interval == 0):\n",
    "                    rmse, auc, r2, acc, all_logits = run_epoch(session, mtest, test_students, item_diff, max_stu, cluster, tf.no_op(), max_skills, \"test\", j, verbose=True)\n",
    "                    print(\" Epoch: %d Test Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f \\t acc: %.3f \\n\" % ( j, rmse, auc, r2, acc))\n",
    "\n",
    "                    j+=1\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-71790a01d55a>:6: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-a81e64504f91>:61: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-a81e64504f91>:66: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-a81e64504f91>:70: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:From C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      " Epoch: 1 Train Metrics:\n",
      " rmse: 0.452 \t auc: 0.677 \t r2:  0.089 \t acc: 0.689 \n",
      "\n",
      " Epoch: 2 Train Metrics:\n",
      " rmse: 0.425 \t auc: 0.768 \t r2:  0.196 \t acc: 0.724 \n",
      "\n",
      " Epoch: 3 Train Metrics:\n",
      " rmse: 0.421 \t auc: 0.777 \t r2:  0.211 \t acc: 0.731 \n",
      "\n",
      " Epoch: 4 Train Metrics:\n",
      " rmse: 0.419 \t auc: 0.780 \t r2:  0.216 \t acc: 0.733 \n",
      "\n",
      " Epoch: 5 Train Metrics:\n",
      " rmse: 0.418 \t auc: 0.782 \t r2:  0.220 \t acc: 0.734 \n",
      "\n",
      " Epoch: 1 Test Metrics:\n",
      " rmse: 0.413 \t auc: 0.798 \t r2: 0.247 \t acc: 0.743 \n",
      "\n",
      " Epoch: 6 Train Metrics:\n",
      " rmse: 0.418 \t auc: 0.783 \t r2:  0.223 \t acc: 0.736 \n",
      "\n",
      " Epoch: 7 Train Metrics:\n",
      " rmse: 0.417 \t auc: 0.784 \t r2:  0.225 \t acc: 0.736 \n",
      "\n",
      " Epoch: 8 Train Metrics:\n",
      " rmse: 0.417 \t auc: 0.785 \t r2:  0.227 \t acc: 0.737 \n",
      "\n",
      " Epoch: 9 Train Metrics:\n",
      " rmse: 0.416 \t auc: 0.786 \t r2:  0.228 \t acc: 0.737 \n",
      "\n",
      " Epoch: 10 Train Metrics:\n",
      " rmse: 0.416 \t auc: 0.786 \t r2:  0.228 \t acc: 0.738 \n",
      "\n",
      " Epoch: 2 Test Metrics:\n",
      " rmse: 0.411 \t auc: 0.801 \t r2: 0.253 \t acc: 0.745 \n",
      "\n",
      " Epoch: 11 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.788 \t r2:  0.231 \t acc: 0.739 \n",
      "\n",
      " Epoch: 12 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.788 \t r2:  0.231 \t acc: 0.739 \n",
      "\n",
      " Epoch: 13 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.789 \t r2:  0.232 \t acc: 0.740 \n",
      "\n",
      " Epoch: 14 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.788 \t r2:  0.231 \t acc: 0.739 \n",
      "\n",
      " Epoch: 15 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.789 \t r2:  0.232 \t acc: 0.740 \n",
      "\n",
      " Epoch: 3 Test Metrics:\n",
      " rmse: 0.410 \t auc: 0.803 \t r2: 0.256 \t acc: 0.747 \n",
      "\n",
      " Epoch: 16 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.790 \t r2:  0.234 \t acc: 0.740 \n",
      "\n",
      " Epoch: 17 Train Metrics:\n",
      " rmse: 0.415 \t auc: 0.790 \t r2:  0.234 \t acc: 0.740 \n",
      "\n",
      " Epoch: 18 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.790 \t r2:  0.235 \t acc: 0.740 \n",
      "\n",
      " Epoch: 19 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.791 \t r2:  0.236 \t acc: 0.741 \n",
      "\n",
      " Epoch: 20 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.791 \t r2:  0.236 \t acc: 0.741 \n",
      "\n",
      " Epoch: 4 Test Metrics:\n",
      " rmse: 0.410 \t auc: 0.804 \t r2: 0.257 \t acc: 0.748 \n",
      "\n",
      " Epoch: 21 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.791 \t r2:  0.237 \t acc: 0.741 \n",
      "\n",
      " Epoch: 22 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.792 \t r2:  0.237 \t acc: 0.741 \n",
      "\n",
      " Epoch: 23 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.792 \t r2:  0.237 \t acc: 0.741 \n",
      "\n",
      " Epoch: 24 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.792 \t r2:  0.238 \t acc: 0.742 \n",
      "\n",
      " Epoch: 25 Train Metrics:\n",
      " rmse: 0.414 \t auc: 0.792 \t r2:  0.238 \t acc: 0.742 \n",
      "\n",
      " Epoch: 5 Test Metrics:\n",
      " rmse: 0.410 \t auc: 0.804 \t r2: 0.258 \t acc: 0.748 \n",
      "\n",
      " Epoch: 26 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.792 \t r2:  0.239 \t acc: 0.742 \n",
      "\n",
      " Epoch: 27 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.793 \t r2:  0.239 \t acc: 0.742 \n",
      "\n",
      " Epoch: 28 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.792 \t r2:  0.239 \t acc: 0.742 \n",
      "\n",
      " Epoch: 29 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.793 \t r2:  0.240 \t acc: 0.742 \n",
      "\n",
      " Epoch: 30 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.794 \t r2:  0.241 \t acc: 0.743 \n",
      "\n",
      " Epoch: 6 Test Metrics:\n",
      " rmse: 0.409 \t auc: 0.805 \t r2: 0.260 \t acc: 0.749 \n",
      "\n",
      " Epoch: 31 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.793 \t r2:  0.241 \t acc: 0.742 \n",
      "\n",
      " Epoch: 32 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.794 \t r2:  0.242 \t acc: 0.744 \n",
      "\n",
      " Epoch: 33 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.242 \t acc: 0.744 \n",
      "\n",
      " Epoch: 34 Train Metrics:\n",
      " rmse: 0.413 \t auc: 0.794 \t r2:  0.241 \t acc: 0.743 \n",
      "\n",
      " Epoch: 35 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.242 \t acc: 0.743 \n",
      "\n",
      " Epoch: 7 Test Metrics:\n",
      " rmse: 0.409 \t auc: 0.806 \t r2: 0.261 \t acc: 0.748 \n",
      "\n",
      " Epoch: 36 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.244 \t acc: 0.744 \n",
      "\n",
      " Epoch: 37 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.243 \t acc: 0.744 \n",
      "\n",
      " Epoch: 38 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.243 \t acc: 0.743 \n",
      "\n",
      " Epoch: 39 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.796 \t r2:  0.245 \t acc: 0.744 \n",
      "\n",
      " Epoch: 40 Train Metrics:\n",
      " rmse: 0.412 \t auc: 0.795 \t r2:  0.244 \t acc: 0.745 \n",
      "\n",
      " Epoch: 8 Test Metrics:\n",
      " rmse: 0.408 \t auc: 0.807 \t r2: 0.262 \t acc: 0.750 \n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saw\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
